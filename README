* DaCapo and Cassandra/YCSB benchmarks on Xen4Tess(XARC)
*
********************************************************************************

INTRODUCTION
********************************************************************************

This document contains descriptions and usage of the scripts used to run multiple concurrent instances (each in its own domain) of the DaCapo and Cassandra/YCSB benchmark on Xen4Tess. Instances are run as OSv (http://osv.io/) images on top of the Xen4Tess hypervisor.

FILES AND THEIR DESCRIPTIONS
********************************************************************************

./dacapo_converge.py: Runs the DaCapo benchmark suite locally on the machine (NOT WITHIN A DOMAIN) to determine the average amount of iterations needed to warmup each of the DaCapo benchmarks until performance converges.
./dacapo_convergences.json: The average amount of interations needed to warmup each of the DaCapo benchmarks stored in json format.
./dacapo_min_heap.json: DEPRECIATED. The min heapsize needed for each of the DaCapo benchmarks. The current heapsizes are inaccurate so please do not refer to them for now.
./parse_dacapo.py: Parses results from the DaCapo experiments and generates graphs.
./parse_dacapo_jit.py: Parses results from the DaCapo JIT experiments and generates graphs.
./run_dacapo.py: Standard DaCapo Experiment. Run script to startup multiple concurrent instances of the DaCapo benchmark suite all running the same benchmark. Benchmark iteration runtimes are outputed for each domain.
./run_dacapo_jit.py: JIT Experiment. Run script to startup multiple concurrent instances of the DaCapo benchmark to warmup in parallel. After warmup, all but one domain are destroyed. The remaining domain is run to completion to output benchmark iteration runtimes.
./setup-gangsched-cpupool.sh: A bash script intended to easily create a new gang-scheduled cpupool within Xen.
./run_cassandra_ycsb.py: Script for running the cassandra experiment. It will first start cassandra domains, and once they are ready, it will launch ycsb domains and send workload requests. 

- NOTE: The following files are adapted from the run scripts found at https://github.com/cloudius-systems/osv/tree/master/scripts. These run scripts are used to start up domains on a Xen hypervisor

./scripts/imgedit.py: Edits the command line to execute upon startup for an OSv image using qemu-nbd. Uses nbd_client.py to edit the image. 
./scripts/nbd_client.py: Client to connect to qemu-nbd. Assumes qemu-nbd is found on port 10809.
./scripts/run.py: Main run script. When using the Xen hypervisor, this run script creates a temporary file containing the domain config and uses the xl toolset to launch the domain. 

RUNNING DACAPO EXPERIMENTS
********************************************************************************

To run the DaCapo experiements, the run_dacapo.py script is used. This script launches multiple domains running DaCapo in parallel. This script also uses the xenalyze and xentrace tools to profile CPU usage in Xen. The important options to run_dacapo.py are as follows

  -b BENCHMARK, --benchmark BENCHMARK
                        Choose which benchmark(s) to run. Currently must be from the following list: avrora, h2, jython, luindex, lusearch, xalan. The other benchmarks do not seem to run stably on OSv. Can pass in a comma delimited string of benchmarks as well.
  --startjvms STARTJVMS
                        Starting number of JVMs to run in parallel. 
  -n NUMJVMS, --numjvms NUMJVMS
                        Max numbers of JVMs to run in parallel. If startjvms is 1 and numjvms is 16, then we will run experiments with 1, 2, 4, 8, and 16 domains running in parallel.   
  -r RESULTSDIR, --resultsdir RESULTSDIR
                        Directory to store results to.
  --startheap STARTHEAP
                        Starting Heapsize for our benchmarks.
  -p MAXHEAP, --maxheap MAXHEAP
                        Max Heapsize for our benchmarks. If startheap is 512 and maxheap is 2048, then we will run experiments  with JVMS containing 512, 1024, and 2048MB heapsizes.
  -v, --verbose         Be more verbose
  -s, --stdout          Output to stdout rather than to results directory.
  -x, --xen             Whether or not to run this experiment on Xen.
  -g, --gangscheduled   Whether or not gang scheduling is used
  -i IMAGE, --image IMAGE
                        Location of the OSv image with DaCapo on it. 
  -m MEMSIZE, --memsize MEMSIZE
                        Specify memory: ex. 1G, 2G, ... to allocate to each domain.
  -c VCPUS, --vcpus VCPUS
                        Specify number of vCPUs to allocate to each domain
  -a CPUS, --cpus CPUS  Which CPU's to pin to for Xen
  --safe                Run in 'Safe' Mode (don't rerun and overwrite tests
                        which we already have results for)
  --cpupool CPUPOOL     Which Xen cpupool to use
  --pausefirst          Whether or not to pause all the domains until all of them begin running the JVM
  --pauseafterwarmup    Whether or not to pause all domains until all of them are done warming up.

- NOTE: Here is an example command that can be run:

sudo ./run_dacapo.py -b avrora --startjvms=4 -n 16 -r test --startheap=64 --maxheap=256 -v -x -i ../osv_images/dacapo.qemu -m 512M -c 6 --cpupool=CreditSchedPool â€”pausefirst.

- This runs the avrora benchmark on 4, 8, and 16 JVMs in parallel with heapsizes of 64, 128, 256MB. This results in 9 combinations of heapsize and JVM size. We turn the xen and verbose flags on and output data into the test folder. Each domain is allocated 512M of memory and 6 vCPUs and is run on a CreditSchedPool. We choose to pause the domains until all domains intialize their JVMs.

To run the JIT experiment, run_dacapo_jit.py is used. This script has similar options to it. One additional option is given.

  --iterations ITERATIONS
                        How many iterations of the experiment to run.

The JIT experiment warmups JVM's in parallel and then measures how long a single JVM takes to complete the benchmark when running alone. The iterations flag allows the user to run this experiement multiple times for multiple data points.


RUNNING CASSANDRA/YCSB EXPERIMENTS
********************************************************************************

To run the Cassandra experiements, the run_cassandra_ycsb.py script is used. This script launches multiple Cassandra domains in parallel, and start multiple concurrent ycsb domains afterwards. For Cassandra, we use 2.1.2. A detailed introduction of ycsb(code, workloads, etc...) can be found at https://github.com/brianfrankcooper/YCSB/wiki. The original ycsb doesn't work well with Cassandra 2.1.2 and has no cql support. We adopt the patch from https://github.com/cmatser/YCSB/tree/master/cassandra. 

The important options to run_cassandra_ycsb.py are as follows

optional arguments:
  -h, --help            show this help message and exit
  --startjvms STARTJVMS
                        starting amount of JVM's to test on
  -n NUMJVMS, --numjvms NUMJVMS
                        max amount of JVM's to test on
  -r RESULTSDIR, --resultsdir RESULTSDIR
                        where to store results
  --startjvm STARTJVM   starting numebr of jvms
  --startheap STARTHEAP
                        starting heap size
  -p MAXHEAP, --maxheap MAXHEAP
                        max heap size
  -v, --verbose         be more verbose
  -s, --stdout          Output to stdout rather than to results dir
  -x, --xen             whether or not to run on xen
  -g, --gangscheduled   whether or not the version of xen has gang scheduling
  -ci CASSANDRA_IMAGE, --cassandra-image CASSANDRA_IMAGE
                        location of the osv image with cassandra on it
  -yi YCSB_IMAGE, --ycsb-image YCSB_IMAGE
                        location of the osv image with ycsb on it
  -m MEMSIZE, --memsize MEMSIZE
                        specify memory, in MB, ex. 1000, 2000, ...
  -c VCPUS, --vcpus VCPUS
                        specify number of vcpus
  -l, --losetup         Whether or not use loop devices as disk image.
  --ycsb-home YCSB_HOME
                        path to the ycsb home
  --cassandra-home CASSANDRA_HOME
                        path to the cassandra home
  --workload WORKLOAD   the workload file to run by ycsb
  -a CPUS, --cpus CPUS  Which CPU's to pin to for Xen
  --cpupool CPUPOOL     Which Xen cpupool to use
  -nc NUM_CLUSTERS, --num-clusters NUM_CLUSTERS
                        the number of clusters to run
  --init-cql INIT_CQL   the cql file to init cassandra for testing
  --ycsb-cmd YCSB_CMD   extra ycsb arguments
  --clean CLEAN         clean all cassandra domains

  For Xen networking, a virtual bridge needed to be manually set up and pass the gateway address to strings in the variable "defaultgw" on line 25 and 28. The script use 172.16.2.* to assign static ip for ycsb and cassandra domains.

  Note:
  When building cassandra images, since Cassandra uses a config file to set up the rpc_address, please make sure that they don't conflict. Otherwise the domain will be forced down during start up.

  An example command:
  sudo python ./run_cassandra_ycsb.py -x -v -n 4 -m 2048 -ci /scratch/teamosv/osv_images/cassandra.qemu -yi /scratch/teamosv/osv_images/ycsb.qemu -c 6 -a all --cpupool CreditSchedPool -r results_ycsb10 --cassandra-home ~/cassandra --init-cql init_ycsb_testtable.cql

  -x means run on Xen, -v means verbose, -n 4 means run 4 domains, -m 2048 means each Cassandra domain has 2048 MB memory, -ci is the path to the cassandra qemu images. If you specify 4 domains, and give path to image name xxx.qemu, the script will look for xxx.qemu_1, xxx.qemu_2, xxx.qemu_3, xxx.qemu_4. -yi gives the path to the ycsb image. -c 6 means run on 6 virtual cpus, -a all means use all cpus in the specified pool, --cpupool CreditSchedPool means run on the CreditSchedPool, -r results_ycsb10 means put all the results in the directory 'results_ycsb10', --cassandra-home ~/cassandra gives the path to cassandra, which is used to send cql to init the tables in Cassandra domains for YCSB requests, --init-cql init_ycsb_testtable.cql gives the file of the init cql to run. Currently the content is

    create keyspace ycsb
    WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor': 1 };

    use ycsb;

    create table usertable (
        y_id varchar primary key,
        field0 varchar,
        field1 varchar,
        field2 varchar,
        field3 varchar,
        field4 varchar,
        field5 varchar,
        field6 varchar,
        field7 varchar,
        field8 varchar,
        field9 varchar);

  This is compatible with the Cassandra table schema inside ycsb code.

PARSING EXPERIMENT DATA FOR GRAPHS
********************************************************************************

DACAPO

  -t TYPE, --type TYPE  
                        Select a specific type of graph to generate. Omitting this option results in generating the default graph type of mean total runtimes. Currently supports the following 4 additional graph types: slowdown, gc, cdf. These plot runtime slowdown ratios compared to one JVM, slowdown in terms of total garbage collection time, and the cumulative distribution function of operations completed over time respectively.
  -x, --xen
                        Parse Xen results instead of linux
  -r RESULTSDIR, --resultsdir RESULTSDIR
                        Specify the directory containing the results to be parsed
  -o OUTPUTDIR, --outputdir OUTPUTDIR
                        Specify the directory to in which to save the generated graphs. If this option is omitted, the default behavior is to start up python to show graphs in separate windows (requires windowing system).
  -e EXTENSION, --extension EXTENSION
                        If the OUTPUTDIR option is provided, this specifies the file type the images should be saved in. The default is eps.
  -b BENCHMARK, --benchmark BENCHMARK
                        Choose which benchmark(s) to run. Currently must be from the following list: avrora, h2, jython, luindex, lusearch, xalan.

Sample commands:

python parse_dacapo.py -x -r results_directory

python parse_dacapo.py -x -t slowdown -r results_directory -o output_directory

YCSB

  -t TYPE, --type TYPE  
                        Select a specific type of graph to generate. Omitting this option results in generating every graph type. Currently supports the following graph type options: 
                          'ovr_runtime' - Overall runtime of the benchmarks for different JVM counts
                          'ovr_thruput' - Overall throughput
                          'r_latency' - Mean read latency
                          'r_95_latency' - 95th percentile read latency
                          'r_99_latency' - 99th percentile read latency
                          'u_latency' - Mean update latency
                          'u_95_latency' - 95th percentile update latency
                          'u_99_latency' - 99th percentile update latency
                          'rw_latency' - Mean read-write latency
                          'rw_95_latency' - 95th percentile read-write latency
                          'rw_99_latency' - 99th percentile read-write latency
  -x, --xen
                        Parse Xen results instead of linux
  -r RESULTSDIR, --resultsdir RESULTSDIR
                        Specify the directory containing the results to be parsed
  -o OUTPUTDIR, --outputdir OUTPUTDIR
                        Specify the directory to in which to save the generated graphs. If this option is omitted, the default behavior is to start up python to show graphs in separate windows (requires windowing system).
  -e EXTENSION, --extension EXTENSION
                        If the OUTPUTDIR option is provided, this specifies the file type the images should be saved in. The default is eps.

Sample commands:

python parse_ycsb.py -x -r ycsb_results_directory

python parse_ycsb.py -x -t rw_latency -r ycsb_results_directory -o output_directory

